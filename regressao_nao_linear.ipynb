{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from module import generate_data, calculate_error, non_linear_target_function, linear_regression, pocket_pla, transform_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non Linear Regression\n",
    "Do 1000 executions of:\n",
    "1. Choose a target function.\n",
    "2. Generate 1000 points data from the distribution $(X \\in [1, -1] \\times [1, -1])$ and classify it with the target function.\n",
    "3. Randomly select 10% of the points and invert their labels\n",
    "4. Run the Linear Regression algorithm and the Linear Regression algorithm with transformation\n",
    "5. Evaluate on a set of 1000 test points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Linear Regression without any transformation, using the vector of attributes(1, x2, x2) to find the weight w. What is the approximate classification value of the average within-sample error Ein?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ein mean: 0.50624\n",
      "Ein std: 0.04278037400491025\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Experimento\n",
    "def experiment(num_runs, num_points):\n",
    "    ein_list = []\n",
    "    for _ in range(num_runs):\n",
    "        nlg_tg = non_linear_target_function()\n",
    "        X, y = generate_data(num_points, nlg_tg, noise_ratio=0.1)\n",
    "        w = linear_regression(X, y) \n",
    "        ein = calculate_error(X, y, w)\n",
    "        ein_list.append(ein)\n",
    "\n",
    "    return np.mean(ein_list), np.std(ein_list)\n",
    "\n",
    "# Parâmetros\n",
    "num_runs = 1000\n",
    "num_points = 1000\n",
    "\n",
    "# Executar o experimento\n",
    "mean_ein, std_ein = experiment(num_runs, num_points)\n",
    "print(f\"Ein mean: {mean_ein}\")\n",
    "print(f\"Ein std: {std_ein}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, transform the N = 1000 training data following the non-linear attribute vector $(1, x_1, x_2, x_1x_2, x_1^2, x_2^2)$. Find the vector we that corresponds to the solution of the Linear Regression. Which of the following hypotheses is closest to the one you found? Evaluate the average result obtained after 1000 runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pesos médios após 1000 execuções: [-4.96454501e-01 -4.96454501e-01 -8.84360381e-05 -6.98526674e-04\n",
      "  2.06003453e-03  1.55825422e+00  1.55536331e+00]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (7,) (6,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\mwuillau\\Documents\\UFRJ\\IntelComp\\intel-comp\\regressao_nao_linear.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 33>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mwuillau/Documents/UFRJ/IntelComp/intel-comp/regressao_nao_linear.ipynb#W1sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39m# Comparar pesos médios encontrados com as hipóteses fornecidas\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mwuillau/Documents/UFRJ/IntelComp/intel-comp/regressao_nao_linear.ipynb#W1sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39mfor\u001b[39;00m key, hypothesis \u001b[39min\u001b[39;00m hypotheses\u001b[39m.\u001b[39mitems():\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/mwuillau/Documents/UFRJ/IntelComp/intel-comp/regressao_nao_linear.ipynb#W1sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     distance \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39mnorm(weights_mean \u001b[39m-\u001b[39;49m hypothesis)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mwuillau/Documents/UFRJ/IntelComp/intel-comp/regressao_nao_linear.ipynb#W1sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDistância da hipótese \u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00mdistance\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mwuillau/Documents/UFRJ/IntelComp/intel-comp/regressao_nao_linear.ipynb#W1sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39m# Encontre a hipótese mais próxima\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (7,) (6,) "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Experimento\n",
    "def experiment(num_runs, num_points):\n",
    "    weights_list = []\n",
    "\n",
    "    for _ in range(num_runs):\n",
    "        nlg_tg = non_linear_target_function()\n",
    "        X, y = generate_data(num_points, nlg_tg, noise_ratio=0.1)\n",
    "        X_transformed = transform_data(X)\n",
    "        w = linear_regression(X_transformed, y)\n",
    "        weights_list.append(w)\n",
    "\n",
    "    weights_mean = np.mean(weights_list, axis=0)\n",
    "    return weights_mean\n",
    "\n",
    "# Parâmetros\n",
    "num_runs = 1000\n",
    "num_points = 1000\n",
    "\n",
    "# Executar o experimento\n",
    "weights_mean = experiment(num_runs, num_points)\n",
    "print(f\"Pesos médios após 1000 execuções: {weights_mean}\")\n",
    "\n",
    "# Hipóteses fornecidas\n",
    "hypotheses = {\n",
    "    \"a\": np.array([-1, -0.05, 0.08, 0.13, 1.5, 1.5]),\n",
    "    \"b\": np.array([-1, -0.05, 0.08, 0.13, 1.5, 15]),\n",
    "    \"c\": np.array([-1, -0.05, 0.08, 0.13, 15, 1.5]),\n",
    "    \"d\": np.array([-1, -1.5, 0.08, 0.13, 0.05, 0.05]),\n",
    "    \"e\": np.array([-1, -0.05, 0.08, 1.5, 0.15, 0.15]),\n",
    "}\n",
    "\n",
    "# Comparar pesos médios encontrados com as hipóteses fornecidas\n",
    "for key, hypothesis in hypotheses.items():\n",
    "    distance = np.linalg.norm(weights_mean - hypothesis)\n",
    "    print(f\"Distância da hipótese {key}: {distance}\")\n",
    "\n",
    "# Encontre a hipótese mais próxima\n",
    "closest_hypothesis = min(hypotheses, key=lambda k: np.linalg.norm(weights_mean - hypotheses[k]))\n",
    "print(f\"Hipótese mais próxima: {closest_hypothesis}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qual o valor mais próximo do erro de classificação fora-de-amostra Eout de sua hipótese na questão anterior? (Estime-o gerando um novo conjunto de 1000 pontos e usando 1000 execuções diferentes, como antes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(num_runs, num_points_train, num_points_test):\n",
    "    eout_list = []\n",
    "\n",
    "    for _ in range(num_runs):\n",
    "        # Dados de treinamento\n",
    "        X_train, y_train = generate_data(num_points_train, noise_ratio=0.1)\n",
    "        X_train_transformed = transform_data(X_train)\n",
    "        w = linear_regression(X_train_transformed, y_train)\n",
    "        \n",
    "        # Dados de teste\n",
    "        X_test, y_test = generate_data(num_points_test, noise_ratio=0.0)\n",
    "        X_test_transformed = transform_data(X_test)\n",
    "        eout = calculate_error(X_test_transformed, y_test, w)\n",
    "        \n",
    "        eout_list.append(eout)\n",
    "\n",
    "    return np.mean(eout_list), np.std(eout_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Média de E_out: 0.032398\n",
      "Desvio padrão de E_out: 0.011196320645640693\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# Experimento\n",
    "def experiment(num_runs, num_points_train, num_points_test):\n",
    "    eout_list = []\n",
    "\n",
    "    for _ in range(num_runs):\n",
    "        # Dados de treinamento\n",
    "        X_train, y_train = generate_data(num_points_train)\n",
    "        X_train_transformed = transform_data(X_train)\n",
    "        w = linear_regression(X_train_transformed, y_train)\n",
    "        \n",
    "        # Dados de teste\n",
    "        X_test, y_test = generate_data(num_points_test, noise_ratio=0.0)\n",
    "        X_test_transformed = transform_data(X_test)\n",
    "        eout = calculate_eout(X_test_transformed, y_test, w)\n",
    "        \n",
    "        eout_list.append(eout)\n",
    "\n",
    "    return np.mean(eout_list), np.std(eout_list)\n",
    "\n",
    "# Parâmetros\n",
    "num_runs = 1000\n",
    "num_points_train = 1000\n",
    "num_points_test = 1000\n",
    "\n",
    "# Executar o experimento\n",
    "mean_eout, std_eout = experiment(num_runs, num_points_train, num_points_test)\n",
    "print(f\"Média de E_out: {mean_eout}\")\n",
    "print(f\"Desvio padrão de E_out: {std_eout}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
