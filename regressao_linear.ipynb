{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from module import generate_target_function, generate_data, pla, calculate_error, linear_target_function, linear_regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "Do 1000 executions of:\n",
    "1. Choose a target function.\n",
    "2. Generate data from the distribution $(X \\in [1, -1] \\times [1, -1])$ and classify it with the target function.\n",
    "3. Run the Linear Regression algorithm\n",
    "4. Evaluate on a set of 1000 test points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimento\n",
    "def experiment_lr(num_runs, num_points_train, num_points_test):\n",
    "    ein_list = []\n",
    "    eout_list = []\n",
    "\n",
    "    for _ in range(num_runs):\n",
    "        a, b, c = generate_target_function()\n",
    "        lg_tg = linear_target_function(a, b, c)\n",
    "        X_train, y_train = generate_data(num_points_train, lg_tg)\n",
    "        w = linear_regression(X_train, y_train)\n",
    "        ein = calculate_error(X_train, y_train, w)\n",
    "        ein_list.append(ein)\n",
    "\n",
    "        # Gerar novos pontos de teste\n",
    "        X_test, y_test = generate_data(num_points_test, lg_tg)\n",
    "        eout = calculate_error(X_test, y_test, w)\n",
    "        eout_list.append(eout)\n",
    "\n",
    "    return np.mean(ein_list), np.std(ein_list), np.mean(eout_list), np.std(eout_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consider N = 100. Use Linear Regression to find g and calculate Ein, the fraction of points within the sample that were classified incorrectly Repeat the experiment 1000 times. Which of the values below is closest to the average Ein?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_lg = experiment_lr(1000, 100, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ein mean:  0.03851\n",
      "Ein std:  0.030493932183304928\n"
     ]
    }
   ],
   "source": [
    "print('Ein mean: ', results_lg[0])\n",
    "print('Ein std: ', results_lg[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, generate 1000 new points and use them to estimate the Eout of the g's you found in item. Again, perform 1000 runs. Which of the values below is closest to the average Eout?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eout mean:  0.048468000000000004\n",
      "Eout std:  0.03204788567128883\n"
     ]
    }
   ],
   "source": [
    "print('Eout mean: ', results_lg[2])\n",
    "print('Eout std: ', results_lg[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Now, consider N = 10. After finding the weights using Linear Regression, use them as an initial vector for the Perceptron Learning Algorithm (PLA). Run the PLA until it converges on a final vector of weights that perfectly separates the within-sample points. Among options below, which is closest to the average number of iterations (over 1000 runs) that the PLA takes to converge?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_rg_pla(num_runs, num_points_train):\n",
    "    iterations_list = []\n",
    "\n",
    "    for _ in range(num_runs):\n",
    "        a, b, c = generate_target_function()\n",
    "        lg_tg = linear_target_function(a, b, c)\n",
    "        X_train, y_train = generate_data(num_points_train, lg_tg)\n",
    "        w = linear_regression(X_train, y_train)\n",
    "        w, iterations = pla(X_train, y_train, w)\n",
    "        iterations_list.append(iterations)\n",
    "\n",
    "    return np.mean(iterations_list), np.std(iterations_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations mean: 3.465\n",
      "Iterations std: 11.045215027331972\n"
     ]
    }
   ],
   "source": [
    "# Par√¢metros\n",
    "num_runs = 1000\n",
    "num_points_train = 10\n",
    "\n",
    "# Executar o experimento\n",
    "mean_iterations, std_iterations = experiment_rg_pla(num_runs, num_points_train)\n",
    "print(f\"Iterations mean: {mean_iterations}\")\n",
    "print(f\"Iterations std: {std_iterations}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
